{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 텍스트분석 + 싸이킷런 파이프라인!!\n",
    "## 텍스트분석 할때 많이 하는 것\n",
    "- 1) 문서(문장)을 단어나 형태소로 쪼갬\n",
    "    - 영어는 띄어쓰기 단위로 분리\n",
    "    - 한글은 형태소를 분석을 통해 형태소 단위로 분리\n",
    "    - konlpy패키지에 형태소 분석기를 사용해야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 명사만 추출하는 함수를 만들어 봅시다\n",
    "\n",
    "from konlpy.tag import Twitter\n",
    "\n",
    "def get_noun(text):\n",
    "    tokenizer = Twitter()\n",
    "    nouns = tokenizer.nouns(text)\n",
    "    return [n for n in nouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['떳', '떳', '비행기', '높이', '높이', '우리', '비행기']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_noun(\"떳다 떳다 비행기 날아라 날아라 높이 높이 날아라 우리 비행기\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2) 문서(단어)를 벡터로 바꿈\n",
    "    - 문서에 단어가 발생한 빈도 수를 세서 정수 또는 0이나1로 표기된 매트릭스를 만듬\n",
    "    - 단어의 발생 빈도에 가중치를 부여하여 수치를 변환함 : TF-IDF\n",
    "    - 뉴럴넷을 활용해(임베딩) 벡터로 바꿈 : word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 내의 단어빈도를 파악하고 매트릭스로 만듬\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts = [\"깊은 산속 옹달샘 누가와서 먹나요 토끼?\",\n",
    "         \"산토끼 토끼야 어디로 가느냐 깡총깡총 뛰면서\",\n",
    "         \"푸른하늘 은하수 하얀 쪽배엔 계수나무 한나무 토끼 한마리\",\n",
    "         \"나비야 나비야 이리날아 오너라 호랑나비 흰나비 이리날아 오너라\"]\n",
    "\n",
    "cv = CountVectorizer(tokenizer=get_noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x19 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 21 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['계수나무',\n",
       " '나무',\n",
       " '나비',\n",
       " '나비야',\n",
       " '날',\n",
       " '누가',\n",
       " '마리',\n",
       " '산속',\n",
       " '산토끼',\n",
       " '어디',\n",
       " '오너',\n",
       " '옹달샘',\n",
       " '와',\n",
       " '은하수',\n",
       " '쪽배',\n",
       " '토끼',\n",
       " '푸른',\n",
       " '하늘',\n",
       " '호랑나비']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 케라스에는 유사한 함수가 있음\n",
    "- from keras.preprocessing.text import Tokenizer\n",
    "- 근데 한국어 형태소 분석기를 토크나이저에 적용할 수 없음\n",
    "- 미리 형태소 분석해놓은 다음에 사용할 수는 있음..그럼 한글은 그냥 사이킷런 CountVectorizer써야겠군"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "texts = [\"깊은 산속 옹달샘 누가와서 먹나요 토끼?\",\n",
    "         \"산토끼 토끼야 어디로 가느냐 깡총깡총 뛰면서\",\n",
    "         \"푸른하늘 은하수 하얀 쪽배엔 계수나무 한나무 토끼 한마리\",\n",
    "         \"나비야 나비야 이리날아 오너라 호랑나비 흰나비 이리날아 오너라\",\n",
    "         \"호랑나비 한마리가 꽃밭에 앉았는데\"]\n",
    "\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'가느냐': 14,\n",
       " '계수나무': 21,\n",
       " '깊은': 6,\n",
       " '깡총깡총': 15,\n",
       " '꽃밭에': 26,\n",
       " '나비야': 2,\n",
       " '누가와서': 9,\n",
       " '뛰면서': 16,\n",
       " '먹나요': 10,\n",
       " '산속': 7,\n",
       " '산토끼': 11,\n",
       " '앉았는데': 27,\n",
       " '어디로': 13,\n",
       " '오너라': 4,\n",
       " '옹달샘': 8,\n",
       " '은하수': 18,\n",
       " '이리날아': 3,\n",
       " '쪽배엔': 20,\n",
       " '토끼': 1,\n",
       " '토끼야': 12,\n",
       " '푸른하늘': 17,\n",
       " '하얀': 19,\n",
       " '한나무': 22,\n",
       " '한마리': 23,\n",
       " '한마리가': 25,\n",
       " '호랑나비': 5,\n",
       " '흰나비': 24}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1.,\n",
       "        1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_matrix(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라벨 만들기 : 1-3은 토끼야이기, 4-5는 나비이야기\n",
    "import numpy as np\n",
    "y_data = [0,0,0,1,1]\n",
    "y_data = np.array(y_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TDM을 머신러닝의 데이터로 사용함\n",
    "- 현재 데이터는 0-2까지는 토끼이야기, 3-4는 나비이야기임\n",
    "- 토끼는 0, 나비는 1로 라벨을 정의 하였음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_matrix(texts)\n",
    "y = y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "clf = SGDClassifier()\n",
    "text_clf = clf.fit(X[0:4], y[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\ksg\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.predict(X[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipline은 여러 절차를 한번에 할수 있게 연결해줌\n",
    "## 이 모든걸 한번에!!!\n",
    "- 텍스트를 벡터로 바꾸고\n",
    "- 벡터에 가중치를 적용하고\n",
    "- 분류기에 적용을 하고"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf_svm = Pipeline([('vect', CountVectorizer(tokenizer=get_noun)),\n",
    "                         ('tfidf', TfidfTransformer()),\n",
    "                         ('clf-svm', SGDClassifier(loss='hinge', penalty='l2', alpha=1e-3, n_iter=5, random_state=42))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['깊은 산속 옹달샘 누가와서 먹나요 빌어먹을 토끼',\n",
       " '산토끼 토끼야 어디로 가느냐 깡총깡총 빌어먹을 토끼',\n",
       " '푸른하늘 은하수 하얀 쪽배엔 계수나무 한나무 토끼 빌어먹을 토끼',\n",
       " '나비야 나비야 이리날아 오너라 호랑나비 흰나비 이리날아 오너라',\n",
       " '호랑나비 한마리가 꽃밭에 앉았는데']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 1])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text_clf_svm = text_clf_svm.fit(texts, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 새로운 데이터를 테스트해보자"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts_new = [\"노래하며 춤추는 나는 아름다운 나비\",\n",
    "            \"노래하며 춤추는 나는 빌어먹을 토끼\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf_svm.predict(texts_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 일반 수치형 자료의 경우에도!!!\n",
    "- 결측값 처리하고\n",
    "- 데이터 정규화하고\n",
    "- 파생변수 만들고.... 이런걸 한번에!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "DOWNLOAD_ROOT = \"https://raw.githubusercontent.com/ageron/handson-ml/master/\"\n",
    "HOUSING_PATH = os.path.join(\"datasets\", \"housing\")\n",
    "HOUSING_URL = DOWNLOAD_ROOT + \"datasets/housing/housing.tgz\"\n",
    "\n",
    "def fetch_housing_data(housing_url=HOUSING_URL, housing_path=HOUSING_PATH):\n",
    "    if not os.path.isdir(housing_path):\n",
    "        os.makedirs(housing_path)\n",
    "    tgz_path = os.path.join(housing_path, \"housing.tgz\")\n",
    "    urllib.request.urlretrieve(housing_url, tgz_path)\n",
    "    housing_tgz = tarfile.open(tgz_path)\n",
    "    housing_tgz.extractall(path=housing_path)\n",
    "    housing_tgz.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fetch_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    csv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "    return pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>housing_median_age</th>\n",
       "      <th>total_rooms</th>\n",
       "      <th>total_bedrooms</th>\n",
       "      <th>population</th>\n",
       "      <th>households</th>\n",
       "      <th>median_income</th>\n",
       "      <th>median_house_value</th>\n",
       "      <th>ocean_proximity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.23</td>\n",
       "      <td>37.88</td>\n",
       "      <td>41.0</td>\n",
       "      <td>880.0</td>\n",
       "      <td>129.0</td>\n",
       "      <td>322.0</td>\n",
       "      <td>126.0</td>\n",
       "      <td>8.3252</td>\n",
       "      <td>452600.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.22</td>\n",
       "      <td>37.86</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7099.0</td>\n",
       "      <td>1106.0</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>8.3014</td>\n",
       "      <td>358500.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.24</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1467.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>7.2574</td>\n",
       "      <td>352100.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1274.0</td>\n",
       "      <td>235.0</td>\n",
       "      <td>558.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>5.6431</td>\n",
       "      <td>341300.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.25</td>\n",
       "      <td>37.85</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1627.0</td>\n",
       "      <td>280.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>259.0</td>\n",
       "      <td>3.8462</td>\n",
       "      <td>342200.0</td>\n",
       "      <td>NEAR BAY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   longitude  latitude  housing_median_age  total_rooms  total_bedrooms  \\\n",
       "0    -122.23     37.88                41.0        880.0           129.0   \n",
       "1    -122.22     37.86                21.0       7099.0          1106.0   \n",
       "2    -122.24     37.85                52.0       1467.0           190.0   \n",
       "3    -122.25     37.85                52.0       1274.0           235.0   \n",
       "4    -122.25     37.85                52.0       1627.0           280.0   \n",
       "\n",
       "   population  households  median_income  median_house_value ocean_proximity  \n",
       "0       322.0       126.0         8.3252            452600.0        NEAR BAY  \n",
       "1      2401.0      1138.0         8.3014            358500.0        NEAR BAY  \n",
       "2       496.0       177.0         7.2574            352100.0        NEAR BAY  \n",
       "3       558.0       219.0         5.6431            341300.0        NEAR BAY  \n",
       "4       565.0       259.0         3.8462            342200.0        NEAR BAY  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터와 라벨을 분리\n",
    "- 학습데이터셋과 검증 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = housing[housing._get_numeric_data().columns]\n",
    "X = data.iloc[:,:-1].values\n",
    "y = data.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14448 6192 14448 6192\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "print(len(X_train), len(X_test), len(Y_train), len(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(len(X_train), len(X_test), len(Y_train), len(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 결측값 채우고 정규화하는 함수를 파이프라인으로 연결"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy=\"median\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "num_pipeline = Pipeline([('imputer', Imputer(strategy=\"median\")),\n",
    "                         ('std_scaler', StandardScaler()),])\n",
    "\n",
    "housing_num_tr = num_pipeline.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.64821297, -0.68811806, -1.07853296, ...,  1.53466411,\n",
       "         1.76494143, -0.50715954],\n",
       "       [ 0.6531961 , -0.85669979,  0.98848922, ...,  0.20264384,\n",
       "        -0.11785725, -0.36780162],\n",
       "       [-0.08929041,  0.52473385,  1.62449604, ..., -0.12284318,\n",
       "        -0.06286591, -0.72098999],\n",
       "       ..., \n",
       "       [ 0.6531961 , -0.79582306,  1.06799007, ..., -0.57834812,\n",
       "        -0.54469478, -1.45114921],\n",
       "       [ 1.2013405 , -0.88947957, -1.47603722, ..., -0.09365549,\n",
       "        -0.06286591, -0.65684615],\n",
       "       [-1.31015748,  1.01174774,  0.5114841 , ..., -0.37138083,\n",
       "        -0.29068717, -0.83089789]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_num_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 공식문서에 예시 가져옴\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.datasets import samples_generator\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# generate some data to play with\n",
    "X, y = samples_generator.make_classification(n_informative=5, \n",
    "                                              n_redundant=0, \n",
    "                                              random_state=42)\n",
    "# ANOVA SVM-C\n",
    "anova_filter = SelectKBest(f_regression, k=5)\n",
    "clf = svm.SVC(kernel='linear')\n",
    "anova_svm = Pipeline([('anova', anova_filter), ('svc', clf)])\n",
    "anova_svm.set_params(anova__k=10, svc__C=.1).fit(X, y)\n",
    "                      \n",
    "Pipeline(memory=None,\n",
    "         steps=[('anova', SelectKBest()),\n",
    "                ('svc', SVC())])\n",
    "\n",
    "prediction = anova_svm.predict(X)\n",
    "anova_svm.score(X, y)    \n",
    "\n",
    "# 음 근데 에러나는군"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 연습해보세요!\n",
    "- http://replet.tistory.com/70?category=667742\n",
    "- 데이터랑 코드 : https://drive.google.com/file/d/10oRjhYD_-nwXVfl-aP-i7euMZwjE2yi1/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "참고자료 1 - https://bbengfort.github.io/tutorials/2016/05/19/text-classification-nltk-sckit-learn.html\n",
    "\n",
    "\n",
    "참고자료 2 - http://nadbordrozd.github.io/blog/2016/05/20/text-classification-with-word2vec/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
